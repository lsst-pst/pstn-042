\section{Introduction}

\note{This paper has potential overlap / connections with several other Construction Papers, including the Alert Production (PSTN-038) and Data Release Production (PSTN-039) science verification papers based on commissioning data, as well as several of the Data Management papers, such as ``LSST Data Management Quality Assurance and Reliability Engineering'' (PSTN-023) and ``LSST Data Management System Verification and Validation'' (PSTN-024).
In order to distinguish this paper from the others, the current draft outline takes more of a systems engineering approach, and focuses on technical aspects over scientific aspects.
The two commissioning science verification papers are intended for a science user audience interested in the high-level science performance and data products of the as-built Rubin Observatory, and it is envisioned that those papers will focus on results, rather than methods.
Meanwhile, based on current draft outlines, it seems that the Data Management papers are focusing on the testing methodologies and tools used when developing the science pipelines and Data Management system, with less emphasis on the commissioning and overall system integration specifically.
Therefore, this paper would provide an overview of the systems engineering approach adopted by the project, using the science performance as a specific example of the application.
It does not appear that the other construction papers cover this topic.
One intended audience is the LSST Operations Team that would likely use much of the tooling to monitor system performance over time, and to help evaluate the science quality of data releases.
}

\begin{itemize}

\item Paragraph to introduce the LSST, emphasizing the size/richness/complexity of the dataset, multiple data processing steps, and stringent requirements on systematic uncertainty

\item Paragraph to describe how the science drivers translate to high-level requirements on scientific performance of the survey and flow-down to observatory specifications, written for a systems engineering audience

\item During commissioning, both the datasets and science pipelines are evolving rapidly, and we need a verification framework that will be functional at survey scale

\item Paragraph to briefly describe the verification and validation framework used for commissioning

\item Pointers to other relevant Rubin Observatory construction papers for context

\end{itemize}

\section{Verification Architecture and Tools for System Integration and Commissioning}

(Austin, Gabriele)

\begin{itemize}

\item Integrated modeling approach

\item Description of verification architecture, including concepts of Verification Elements, Test Plans, Test Cases, etc.

\item Description of verification tooling, e.g., MagicDraw, Syndeia, and JIRA

\item Description of verification workflow

\item Description of visualization and status tooling

\end{itemize}

\section{Application to Science Verification and Validation for Commissioning}

(Keith, Andy, Jeff, Simon)

\begin{itemize}

\item Examples to illustrate use of verification architecture described above

\item Description of science verification analysis framework used in commissioning

\item Example results showing summary of verification status during commissioning

\end{itemize}

\section{Discussion}

\begin{itemize}

\item Brief summary of the utility / effectiveness of the verification framework used in commissioning

\item We intend to use this framework during LSST Operations, with the following suggestions for improvement

\item We hope that these strategies might be useful for future projects

\end{itemize}
